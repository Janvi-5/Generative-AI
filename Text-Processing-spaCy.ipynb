{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d7097c-28f7-4894-a6dd-f65ca89007b5",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa4fbb76-aa57-439c-9790-fd2801bf8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the SpaCy library for Natural Language Processing (NLP)\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95290f-b278-496f-977d-df3d61fd2f2b",
   "metadata": {},
   "source": [
    "# 1. Create a Doc object from the file owlcreek.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b03ec7c-e85d-48b0-b754-c0a320fa882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc object created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Loading the SpaCy model (English language model for text analysis)\n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "\n",
    "# Reading the text file and storing its content in a variable\n",
    "with open(\"owlcreek (2).txt\", \"r\") as file:  \n",
    "    text = file.read()  \n",
    "\n",
    "# Processing the text using SpaCy to create a Doc object\n",
    "doc = nlp(text)  \n",
    "\n",
    "# Confirming successful text processing\n",
    "print(\"Doc object created successfully!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc87952-26ee-4947-b38b-3e5dd8927595",
   "metadata": {},
   "source": [
    "# 2. How many tokens are contained in the file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0ed9c9c-bbc4-4b86-84c5-8d8ae540bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the file: 4835\n"
     ]
    }
   ],
   "source": [
    "# Counting the total number of tokens in the processed document\n",
    "num_tokens = len(doc)  \n",
    "\n",
    "# Displaying the total count of tokens in the file\n",
    "print(\"Number of tokens in the file:\", num_tokens)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f1c24-d84a-4f08-a221-6b0f8d0d3e25",
   "metadata": {},
   "source": [
    "# 3. How many sentences are contained in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6b9cc62-d229-446a-ba7c-c1851675f491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the file: 204\n"
     ]
    }
   ],
   "source": [
    "# Counting the total number of sentences in the processed document\n",
    "num_sentences = len(list(doc.sents))  \n",
    "\n",
    "# Displaying the total count of sentences in the file\n",
    "print(\"Number of sentences in the file:\", num_sentences)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe1e56-0334-4c3f-9b60-2546fa0404a8",
   "metadata": {},
   "source": [
    "# 4.Print the second sentence in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61fb0e75-eb0d-462d-a0e6-9451e4748725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sentence: The man's hands were behind\n",
      "his back, the wrists bound with a cord.  \n"
     ]
    }
   ],
   "source": [
    "# Extracting all sentences from the processed document\n",
    "sentences = list(doc.sents)  \n",
    "\n",
    "# Selecting the second sentence from the list of sentences\n",
    "second_sentence = sentences[1]  \n",
    "\n",
    "# Displaying the second sentence from the document\n",
    "print(\"Second sentence:\", second_sentence)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d439364-c125-44ce-995c-ec2378637b86",
   "metadata": {},
   "source": [
    "# 5. For each token in the sentence above, print its text, POS tag, dep tag and lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1896a63e-b87b-4100-935d-acaa7c58610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The, POS: DET, Dep: det, Lemma: the\n",
      "Text: man, POS: NOUN, Dep: poss, Lemma: man\n",
      "Text: 's, POS: PART, Dep: case, Lemma: 's\n",
      "Text: hands, POS: NOUN, Dep: nsubj, Lemma: hand\n",
      "Text: were, POS: AUX, Dep: ROOT, Lemma: be\n",
      "Text: behind, POS: ADP, Dep: prep, Lemma: behind\n",
      "Text: \n",
      ", POS: SPACE, Dep: dep, Lemma: \n",
      "\n",
      "Text: his, POS: PRON, Dep: poss, Lemma: his\n",
      "Text: back, POS: NOUN, Dep: pobj, Lemma: back\n",
      "Text: ,, POS: PUNCT, Dep: punct, Lemma: ,\n",
      "Text: the, POS: DET, Dep: det, Lemma: the\n",
      "Text: wrists, POS: NOUN, Dep: appos, Lemma: wrist\n",
      "Text: bound, POS: VERB, Dep: acl, Lemma: bind\n",
      "Text: with, POS: ADP, Dep: prep, Lemma: with\n",
      "Text: a, POS: DET, Dep: det, Lemma: a\n",
      "Text: cord, POS: NOUN, Dep: pobj, Lemma: cord\n",
      "Text: ., POS: PUNCT, Dep: punct, Lemma: .\n",
      "Text:  , POS: SPACE, Dep: dep, Lemma:  \n"
     ]
    }
   ],
   "source": [
    "# Iterating over each token in the second sentence to extract linguistic features\n",
    "for token in second_sentence:  \n",
    "    print(f\"Text: {token.text}, POS: {token.pos_}, Dep: {token.dep_}, Lemma: {token.lemma_}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b5987-0cd5-4ada-a502-e13ae15fc021",
   "metadata": {},
   "source": [
    "# 6. Write a matcher called 'Swimming' that finds both occurrences of the phrase \"swimming vigorously\" in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52b94065-a572-4175-97ad-ef1b93d3b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Matcher module from SpaCy to search for specific patterns in text\n",
    "from spacy.matcher import Matcher  \n",
    "\n",
    "# Initializing the matcher with the vocabulary from the NLP model\n",
    "matcher = Matcher(nlp.vocab)  \n",
    "\n",
    "# Defining the pattern to match the phrase \"swimming vigorously\" (case insensitive)\n",
    "pattern = [{\"LOWER\": \"swimming\"},{'IS_SPACE': True}, {\"LOWER\": \"vigorously\"}]  \n",
    "matcher.add(\"Swimming\", [pattern])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79adedad-e040-416a-af28-31efc6c89fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12881893835109366681, 1274, 1277), (12881893835109366681, 3609, 3612)]\n"
     ]
    }
   ],
   "source": [
    "# Finding matches in the processed document\n",
    "found_matches = matcher(doc)  \n",
    "\n",
    "# Printing the list of found matches (position details in the text)\n",
    "print(found_matches)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c9ed2d-f0ee-4ddd-b6df-c55de29799be",
   "metadata": {},
   "source": [
    "# 7. Print the text surrounding each found match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ece111cb-bbce-4cc7-918a-dddbc5fcb78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found match in sentence: 'By diving I could evade the bullets and, swimming\n",
      "vigorously, reach the bank, take to the woods and get away home.  '\n",
      "Found match in sentence: 'The hunted man saw all this over his shoulder; he was now swimming\n",
      "vigorously with the current.  '\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    span = doc[start:end]  \n",
    "    \n",
    "    sentence = span.sent.text  \n",
    "    \n",
    "    print(f\"Found match in sentence: '{sentence}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa71189-735f-495b-807a-413a13bb1041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ad669-204a-4f8d-ba43-3df077352de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
